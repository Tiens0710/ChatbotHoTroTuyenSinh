from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import streamlit as st
import random
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from torch.nn.functional import softmax
import json
import pandas as pd
import random
import time
import nltk
from nltk import word_tokenize
from nltk.stem import PorterStemmer

import torch
from torch.utils.data import Dataset
# T·∫£i c√°c resource NLTK
nltk.download('punkt')
nltk.download('stopwords')

# dataset
filename = 'D:\\chatbot\\processed_bert_test_data.json'

# model
model_path = 'D:\\chatbot\\kaggle\\working\\chatbot'

def load_json_file(filename):
    with open(filename, encoding='utf-8') as f:
        file = json.load(f)
    return file

def create_df():
    return pd.DataFrame({'Pattern': [], 'Tag': []})

def extract_json_info(json_file, df):
    for intent in json_file['intents']:
        for pattern in intent['patterns']:
            df.loc[len(df.index)] = [pattern, intent['tag']]
    return df

# Preprocessing
stemmer = PorterStemmer()
stopwords = [
    # C√°c t·ª´ ph·ªï bi·∫øn trong c√¢u h·ªèi
    "l√†", "c·ªßa", "v√†", "c√≥", "cho", "ƒë√¢y", "r·∫±ng", "nh∆∞ng", "t√¥i", 
    "b·∫°n", "v·ªõi", "v·ªÅ", "th√¨", "l·∫°i", "n√†y", "m·ªôt", "nhi·ªÅu", "n√†o",
    
    # T·ª´ li√™n quan ƒë·∫øn gi√°o d·ª•c/tr∆∞·ªùng h·ªçc
    "tr∆∞·ªùng", "ng√†nh", "h·ªçc", "sinh vi√™n", "ƒë·∫°i h·ªçc",
    "tuy·ªÉn sinh", "ƒë√†o t·∫°o", "ch∆∞∆°ng tr√¨nh", "khoa",
    "nƒÉm", "h·ªá", "m√¥n", "ƒëi·ªÉm", "th√¥ng tin", "gi·ªù",
    
    # C·∫•u tr√∫c c√¢u h·ªèi th∆∞·ªùng g·∫∑p
    "nh∆∞ th·∫ø n√†o", "ra sao", "th·∫ø n√†o", "th√¨ sao", "l√† g√¨",
    "·ªü ƒë√¢u", "bao nhi√™u", "khi n√†o", "l√†m sao", "bao gi·ªù",
    
    # T·ª´ ƒë·ªÉ h·ªèi
    "ai", "g√¨", "n√†o", "ƒë√¢u", "sao",
    
    # C√°c t·ª´ n·ªëi
    "m√†", "c√°c", "nh·ªØng", "t·ª´", "t·∫°i", "theo", "sau", "s·∫Ω", "v√†o",
    "do", "nh∆∞", "hay", "c√≤n", "b·ªüi", "v√¨", "m√¨nh", "ƒë·∫øn", "c≈©ng",
    
    # T·ª´ ch·ªâ th·ªùi gian
    "hi·ªán t·∫°i", "hi·ªán nay", "ƒëang", "trong", "nay", "khi",
    
    # T·ª´ ch·ªâ ƒë·ªãnh
    "n√†y", "ƒë√≥", "kia", "·∫•y"
]
# K√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt
ignore_words = [    '?', '!', ',', '.', ':', ';', '-', '', 
    '"', "'", '(', ')', '[', ']', '/', '\\',
    '+', '=', '@', '#', '$', '%', '^', '&', '*']

def preprocess_pattern(pattern):
    words = word_tokenize(pattern.lower())
    stemmed_words = [stemmer.stem(word) for word in words if word not in ignore_words and word not in stopwords]
    return " ".join(stemmed_words)

# Load data
intents = load_json_file(filename)
df = create_df()
df = extract_json_info(intents, df)
df['Pattern'] = df['Pattern'].apply(preprocess_pattern)

# Prepare labels
labels = df['Tag'].unique().tolist()
labels = [s.strip() for s in labels]
num_labels = len(labels)
id2label = {id:label for id, label in enumerate(labels)}
label2id = {label:id for id, label in enumerate(labels)}

# Load model
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)

def predict_with_score(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
    probs = softmax(logits, dim=-1)
    pred_label_id = torch.argmax(probs, dim=-1).item()
    pred_label = model.config.id2label[pred_label_id]
    score = probs[0][pred_label_id].item()
    return {'label': pred_label, 'score': score}

def chat_streamlit():
    st.title("ü§ñ H·ªó Tr·ª£ Sinh Vi√™n")
    st.write("T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n")

    # Initialize chat history
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # Display chat history
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # User input area
    if user_input := st.chat_input("Your message:"):
        # Add user message to chat history
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        # Display user message
        with st.chat_message("user"):
            st.markdown(user_input)

        # Chatbot response
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            # Th·ª±c hi·ªán d·ª± ƒëo√°n
            prediction = predict_with_score(user_input)
            score = prediction['score']
            predicted_label = prediction['label']

            # X·ª≠ l√Ω response
            if score < 0.7:
                bot_response = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
            else:
                # T√¨m intent t∆∞∆°ng ·ª©ng
                matching_intent = None
                for intent in intents['intents']:
                    if intent['tag'].strip() == predicted_label.strip():
                        matching_intent = intent
                        break

                if matching_intent:
                    bot_response = random.choice(matching_intent['responses'])
                else:
                    bot_response = "Xin l·ªói, t√¥i kh√¥ng hi·ªÉu ƒë∆∞·ª£c c√¢u h·ªèi c·ªßa b·∫°n."

            # Hi·ªáu ·ª©ng typing
            typing_effect = ""
            for chunk in bot_response.split(" "):
                typing_effect += chunk + " "
                message_placeholder.markdown(typing_effect)
                time.sleep(0.05)

            # Final response
            message_placeholder.markdown(bot_response)

            # Add response to history
            st.session_state.chat_history.append({
                "role": "assistant", 
                "content": bot_response
            })


def main():
    chat_streamlit()

if __name__ == "__main__":
    main()
